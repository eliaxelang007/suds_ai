{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "\n",
    "%pip install --upgrade pip -q\n",
    "%pip install mypy -q\n",
    "\n",
    "# %pip install numpy matplotlib pandas scipy -q\n",
    "# %pip install setuptools wandb -q\n",
    "\n",
    "%pip install openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/63142182/to-what-extent-does-google-colab-support-python-typing\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "from mypy import api\n",
    "\n",
    "@register_cell_magic\n",
    "def mypy(line, cell):\n",
    "  for output in api.run(['-c', '\\n' + cell] + line.split()):\n",
    "    if output and not output.startswith('Success'):\n",
    "      raise TypeError(output)\n",
    "\n",
    "  get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import (\n",
    "    ChatCompletionMessage,\n",
    "    ChatCompletionSystemMessageParam,\n",
    "    ChatCompletionUserMessageParam,\n",
    "    ChatCompletionAssistantMessageParam,\n",
    "    ChatCompletionToolMessageParam,\n",
    "    ChatCompletionFunctionMessageParam,\n",
    "  )\n",
    "\n",
    "from openai.types.chat.completion_create_params import ResponseFormat\n",
    "from openai._types import NotGiven\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# WANDB = False\n",
    "\n",
    "# if WANDB:\n",
    "#   from wandb.integration.openai import autolog\n",
    "#   autolog({\"project\": \"First RAG App\"})\n",
    "\n",
    "ai_client = OpenAI(api_key=dotenv_values(\"../.env\")[\"OPENAI_API_KEY\"])\n",
    "\n",
    "RawConversationMessage = ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam\n",
    "\n",
    "class Role(Enum):\n",
    "  System = \"system\"\n",
    "  User = \"user\"\n",
    "  Assistant = \"assistant\"\n",
    "  Tool = \"tool\"\n",
    "\n",
    "class AiModel(Enum):\n",
    "  Gpt3_5Turbo = \"gpt-3.5-turbo\"\n",
    "\n",
    "class AssistantTool(Enum):\n",
    "   CodeInterpreter = \"code_interpreter\"\n",
    "   FileSearch = \"file_search\"\n",
    "   Function = \"function\"\n",
    "\n",
    "@dataclass(frozen=True, order=True, slots=True)\n",
    "class Message():\n",
    "  role: Role\n",
    "  content: str\n",
    "\n",
    "  def into(self) -> RawConversationMessage:\n",
    "    return {\"role\": self.role.value, \"content\": self.content} # type:ignore\n",
    "\n",
    "  @classmethod\n",
    "  def from_raw(cls, raw: ChatCompletionMessage) -> \"Message\":\n",
    "    return cls(Role(raw.role), raw.content or \"\")\n",
    "\n",
    "def system(message: str) -> Message:\n",
    "  return Message(Role.System, message)\n",
    "\n",
    "def user(message: str) -> Message:\n",
    "  return Message(Role.User, message)\n",
    "\n",
    "def respond(messages: list[Message], response_format: ResponseFormat | NotGiven=NotGiven(), model: AiModel = AiModel.Gpt3_5Turbo) -> Message:\n",
    "  raw_messages = [message.into() for message in messages]\n",
    "\n",
    "  return Message.from_raw(\n",
    "    ai_client.chat.completions.create(\n",
    "      model=model.value,\n",
    "      messages=raw_messages,\n",
    "      response_format=response_format\n",
    "    ).choices[0].message\n",
    "  )\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class Chatbot:\n",
    "    messages: list[Message] = field(default_factory=list)\n",
    "\n",
    "    def respond(self, response_format: ResponseFormat | NotGiven=NotGiven()) -> Message:\n",
    "        response = respond(self.messages, response_format = response_format)\n",
    "        self.messages.append(response)\n",
    "        return response\n",
    "\n",
    "    def converse(self, chat: Message, response_format: ResponseFormat | NotGiven=NotGiven()) -> Message:\n",
    "        self.messages.append(chat)\n",
    "        return self.respond(response_format=response_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suds_ai-TmJupkLh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
