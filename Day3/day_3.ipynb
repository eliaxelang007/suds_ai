{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "\n",
    "%pip install --upgrade pip -q\n",
    "%pip install mypy -q\n",
    "\n",
    "# %pip install numpy matplotlib pandas scipy -q\n",
    "# %pip install setuptools wandb -q\n",
    "\n",
    "%pip install openai python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/63142182/to-what-extent-does-google-colab-support-python-typing\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "from mypy import api\n",
    "\n",
    "@register_cell_magic\n",
    "def mypy(line, cell):\n",
    "  for output in api.run(['-c', '\\n' + cell] + line.split()):\n",
    "    if output and not output.startswith('Success'):\n",
    "      raise TypeError(output)\n",
    "\n",
    "  get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import (\n",
    "    ChatCompletionMessage,\n",
    "    ChatCompletionSystemMessageParam,\n",
    "    ChatCompletionUserMessageParam,\n",
    "    ChatCompletionAssistantMessageParam,\n",
    "    ChatCompletionToolMessageParam,\n",
    "    ChatCompletionFunctionMessageParam,\n",
    "  )\n",
    "\n",
    "from openai.types.chat.completion_create_params import ResponseFormat\n",
    "from openai._types import NotGiven\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# WANDB = False\n",
    "\n",
    "# if WANDB:\n",
    "#   from wandb.integration.openai import autolog\n",
    "#   autolog({\"project\": \"First RAG App\"})\n",
    "\n",
    "client = OpenAI(api_key=dotenv_values(\"../.env\")[\"OPENAI_API_KEY\"])\n",
    "\n",
    "RawConversationMessage = ChatCompletionSystemMessageParam | ChatCompletionUserMessageParam | ChatCompletionAssistantMessageParam | ChatCompletionToolMessageParam | ChatCompletionFunctionMessageParam\n",
    "\n",
    "class Role(Enum):\n",
    "  System = \"system\"\n",
    "  User = \"user\"\n",
    "  Assistant = \"assistant\"\n",
    "  Tool = \"tool\"\n",
    "\n",
    "class AiModel(Enum):\n",
    "  Gpt3_5Turbo = \"gpt-3.5-turbo\"\n",
    "\n",
    "class AssistantTool(Enum):\n",
    "   CodeInterpreter = \"code_interpreter\"\n",
    "   FileSearch = \"file_search\"\n",
    "   Function = \"function\"\n",
    "\n",
    "@dataclass(frozen=True, order=True, slots=True)\n",
    "class Message():\n",
    "  role: Role\n",
    "  content: str\n",
    "\n",
    "  def into(self) -> RawConversationMessage:\n",
    "    return {\"role\": self.role.value, \"content\": self.content} # type:ignore\n",
    "\n",
    "  @classmethod\n",
    "  def from_raw(cls, raw: ChatCompletionMessage) -> \"Message\":\n",
    "    return cls(Role(raw.role), raw.content or \"\")\n",
    "\n",
    "def system(message: str) -> Message:\n",
    "  return Message(Role.System, message)\n",
    "\n",
    "def user(message: str) -> Message:\n",
    "  return Message(Role.User, message)\n",
    "\n",
    "def respond(messages: list[Message], response_format: ResponseFormat | NotGiven=NotGiven(), model: AiModel = AiModel.Gpt3_5Turbo) -> Message:\n",
    "  raw_messages = [message.into() for message in messages]\n",
    "\n",
    "  return Message.from_raw(\n",
    "    client.chat.completions.create(\n",
    "      model=model.value,\n",
    "      messages=raw_messages,\n",
    "      response_format=response_format\n",
    "    ).choices[0].message\n",
    "  )\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class Chatbot:\n",
    "    messages: list[Message] = field(default_factory=list)\n",
    "\n",
    "    def respond(self, response_format: ResponseFormat | NotGiven=NotGiven()) -> Message:\n",
    "        response = respond(self.messages, response_format = response_format)\n",
    "        self.messages.append(response)\n",
    "        return response\n",
    "\n",
    "    def converse(self, chat: Message, response_format: ResponseFormat | NotGiven=NotGiven()) -> Message:\n",
    "        self.messages.append(chat)\n",
    "        return self.respond(response_format=response_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queued\n",
      "in_progress\n",
      "SyncCursorPage[Message](data=[Message(id='msg_UpOxKHbBoCgkcLoL620tdn4i', assistant_id='asst_d21pbJKTSpxRO644q9LVkEyC', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='How can I assist you with the book \"Frankenstein\" by Mary Shelley? What specific information or questions do you have about the text?'), type='text')], created_at=1722217586, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_8yoJHyGzVZEOTX8IfV2RqWFI', status=None, thread_id='thread_0fjYnYsKPlTLjBaRdBlcg3K8')], object='list', first_id='msg_UpOxKHbBoCgkcLoL620tdn4i', last_id='msg_UpOxKHbBoCgkcLoL620tdn4i', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from pprint import pprint\n",
    "\n",
    "frankenstien_file = client.files.create(\n",
    "    file=open(\"./frankenstien.txt\", \"rb\"),\n",
    "    purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "processed_documents = client.beta.vector_stores.create(\n",
    "    name=\"Frankenstien Documents\"\n",
    ")\n",
    "\n",
    "vector_store_file = client.beta.vector_stores.files.create(\n",
    "  vector_store_id=processed_documents.id,\n",
    "  file_id=frankenstien_file.id\n",
    ")\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Frank(enstien)\",\n",
    "    instructions=\"\"\"\n",
    "You are a librarian who'd like to answer questions about the book Frankenstien by Mary Shelley.\n",
    "The book is included in a file for your reference; \n",
    "when you answer, you cite your source in the book and explain why you're correct.\n",
    "If you don't know the answer, say so.\n",
    "\"\"\",\n",
    "    model=AiModel.Gpt3_5Turbo.value,\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [processed_documents.id]}}\n",
    ")\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=f\"What is the first words Victor Frankenstein speaks?\"\n",
    ")\n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "# Wait for Run to Complete\n",
    "while run.status == \"in_progress\" or run.status == \"queued\":\n",
    "  sleep(1)\n",
    "  print(run.status)\n",
    "  run = client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id\n",
    "  )\n",
    "\n",
    "# Collect Messages from the Thread\n",
    "messages = client.beta.threads.messages.list(\n",
    "  thread_id=thread.id\n",
    ")\n",
    "\n",
    "pprint(messages)\n",
    "\n",
    "client.beta.assistants.delete(assistant.id)\n",
    "client.beta.vector_stores.delete(processed_documents.id)\n",
    "client.files.delete(frankenstien_file.id)\n",
    "client.beta.threads.delete(thread.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suds_ai-TmJupkLh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
